{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from pytorch_lightning.callbacks import ModelCheckpoint\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "import torchio as tio\n",
    "from pathlib import Path\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm\n",
    "import torch.nn.functional as F\n",
    "import os\n",
    "import shutil\n",
    "import csv\n",
    "\n",
    "from segmenter import Segmenter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --------------------- CMD arguments\n",
    "# TODO Use the latest checkpoint\n",
    "lightning_checkpoint = \"C:/Users/denni/Documents/MedicalDecathlon/Logs/lightning_logs/version_1/checkpoints/epoch=4-step=2570.ckpt\"\n",
    "device = \"cuda\"\n",
    "# This is some path for the preprocessed data\n",
    "input_path = \"C:/Users/denni/Documents/fallstudie-ss2024/data/image-repository/1-test2-uni/16-Mr_Hirn-9346A15227666_01/preprocessed/301_314_313_319/\"\n",
    "# This is some path for the outputs.\n",
    "# TODO Find out which file extension to use\n",
    "output_path = \"C:/Users/denni/Documents/MedicalDecathlon/Outputs/version_1/\"\n",
    "output_channel_mapping = {\"background\": 0, \"edema\": 1, \"non_enhancing_and_necrosis\": 2, \"enhancing_tumor\": 3}\n",
    "# --------------------- CMD arguments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 256, 256, 180])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tio.ScalarImage(\"C:/Users/denni/Documents/fallstudie-ss2024/data/image-repository/1-test2-uni/16-Mr_Hirn-9346A15227666_01/preprocessed/301_314_313_319/nifti_t1_norm_register.nii.gz\").data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\denni\\AppData\\Roaming\\Python\\Python39\\site-packages\\pytorch_lightning\\utilities\\parsing.py:199: Attribute 'activation_fn' is an instance of `nn.Module` and is already saved during checkpointing. It is recommended to ignore them using `self.save_hyperparameters(ignore=['activation_fn'])`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject dimension: (4, 240, 240, 155)\n",
      "Input tensor dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "Output dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "tensor(-5.2684, device='cuda:0') tensor(11.4493, device='cuda:0')\n",
      "\n",
      "Input tensor dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "Output dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "tensor(-5.2685, device='cuda:0') tensor(11.4629, device='cuda:0')\n",
      "\n",
      "Input tensor dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "Output dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "tensor(-5.2685, device='cuda:0') tensor(11.4510, device='cuda:0')\n",
      "\n",
      "Input tensor dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "Output dimension: torch.Size([4, 4, 96, 96, 96])\n",
      "tensor(-5.2685, device='cuda:0') tensor(11.4493, device='cuda:0')\n",
      "\n",
      "Input tensor dimension: torch.Size([2, 4, 96, 96, 96])\n",
      "Output dimension: torch.Size([2, 4, 96, 96, 96])\n",
      "tensor(-5.2685, device='cuda:0') tensor(11.4493, device='cuda:0')\n",
      "\n",
      "Prediction shape: torch.Size([3, 240, 240, 155])\n",
      "tensor([17856000,  8928000])\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'label' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Input \u001b[1;32mIn [43]\u001b[0m, in \u001b[0;36m<cell line: 67>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     64\u001b[0m     pred \u001b[38;5;241m=\u001b[39m pred\u001b[38;5;241m.\u001b[39munsqueeze(dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Extend the number of channels of the prediction to 4 if necessary.\u001b[39;00m\n\u001b[1;32m---> 67\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m it \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(pred\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m], \u001b[43mlabel\u001b[49m\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]):\n\u001b[0;32m     68\u001b[0m     pred \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mcat((pred, torch\u001b[38;5;241m.\u001b[39mzeros_like(pred[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)), \u001b[38;5;241m0\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'label' is not defined"
     ]
    }
   ],
   "source": [
    "# Iterate all saved checkpoints, i.e., the model with the metadata and find out which one performs best on the\n",
    "# validation data. That best model will be used to perform more in-depth analysis on the loss and to visualize\n",
    "# the segmented tumors of the model.\n",
    "loss_per_model = []\n",
    "all_losses = []\n",
    "\n",
    "# Switch the model to eval mode\n",
    "# Get the model from the Pytorch Lightning checkpoint\n",
    "# TODO Remove the arguments learning_rate_decay and dropout_probability\n",
    "model = Segmenter.load_from_checkpoint(lightning_checkpoint, learning_rate_decay=1, dropout_probability=0)\n",
    "# Switch to eval mode\n",
    "model.eval()\n",
    "# Switch to the given device\n",
    "model.to(device)\n",
    "\n",
    "# Some minor preprocessing of the images before using our model on it to ensure that they are the same size\n",
    "# as the training data and have the same pixel range.\n",
    "process = tio.Compose([\n",
    "    tio.CropOrPad((240, 240, 155)),\n",
    "    tio.RescaleIntensity((-1, 1))\n",
    "])\n",
    "\n",
    "# Make a tio.Subject out of the images. The other tio utilities are for extracting patches\n",
    "# from the image and then reassembling them to create a valid image. Our model only takes\n",
    "# 96 by 96 by 96 input images, so we have to use this sampling strategy. There is an overlap\n",
    "# of 8 x 8 x 8 pixels. Since we get the images by sequence, we separately apply the process\n",
    "# function per sequence and then concatenate the rescaled tensors into one multi-channel tensor.\n",
    "paths = []\n",
    "# This is the order of sequences as written in dataset.json.\n",
    "for index, seq in enumerate([\"flair\", \"t1_norm\", \"t1c\", \"t2\"]):\n",
    "    paths.append(os.path.join(input_path, f'nifti_{seq}_register.nii.gz'))\n",
    "\n",
    "tensors = [tio.ScalarImage(path).data for path in paths]\n",
    "full_tensor = torch.cat(tensors)\n",
    "raw_subject = tio.Subject({\"MRI\": tio.ScalarImage(tensor=full_tensor)})\n",
    "subject = tio.SubjectsDataset([raw_subject], transform=process)[0]\n",
    "print(\"Subject dimension:\", subject[\"MRI\"].shape)\n",
    "\n",
    "grid_sampler = tio.inference.GridSampler(subject, 96, (8, 8, 8))\n",
    "aggregator = tio.inference.GridAggregator(grid_sampler)\n",
    "patch_loader = torch.utils.data.DataLoader(grid_sampler, batch_size=4)\n",
    "\n",
    "# This is the actual prediction of the segmentation\n",
    "with torch.no_grad():\n",
    "    for patches_batch in patch_loader:\n",
    "        input_tensor = patches_batch[\"MRI\"][\"data\"].to(device)\n",
    "        print(\"Input tensor dimension:\", input_tensor.shape)\n",
    "        locations = patches_batch[tio.LOCATION]\n",
    "        pred = model(input_tensor)\n",
    "        print(\"Output dimension:\", pred.shape)\n",
    "        print(pred.min(), pred.max(), end=\"\\n\\n\")\n",
    "        # We keep adding batches to the aggregator to later collect all the data.\n",
    "        aggregator.add_batch(pred, locations)\n",
    "\n",
    "# The prediction is composed of the patches we have generated before\n",
    "pred = torch.swapaxes(F.one_hot(aggregator.get_output_tensor().argmax(0)).unsqueeze(dim=0), 0, 4).squeeze()\n",
    "\n",
    "print(\"Prediction shape:\", pred.shape)\n",
    "print(torch.bincount(pred.flatten()))\n",
    "\n",
    "# If the first dimension is not a channel dimension, add that dimension. We assume that everything larger than 6\n",
    "# is not a channel dimension.\n",
    "if pred.shape[0] > 6:\n",
    "    pred = pred.unsqueeze(dim=0)\n",
    "    \n",
    "# Extend the number of channels of the prediction to 4 if necessary.\n",
    "for it in range(pred.shape[0], label.shape[0]):\n",
    "    pred = torch.cat((pred, torch.zeros_like(pred[0]).unsqueeze(0)), 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
